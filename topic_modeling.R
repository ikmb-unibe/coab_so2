#####################
## topic modeling ##
####################

ifelse(!require("tidyverse"), install.packages("tidyverse"), require(tidyverse))
ifelse(!require("quanteda"), install.packages("quanteda"), require(quanteda))
ifelse(!require("stm"), install.packages("stm"), require(quanteda))
source("functions/dfm_splitgrams.R")
source("functions/do_preprocessing.R")
source("functions/get_topdocs.R")

# load data
dfm_combined_nd <- url("https://campuscloud.unibe.ch:443/ssf/s/readFile/share/24121/-5995487485071316790/publicLink/dfm_combined_nd.RData")
load(dfm_combined_nd)

# do preprocessing (in that order)
  # all to lower
  # split ngrams
  # removal of features with less than 3 characters
  # removal of stopwords
  # stemming (using Martin Porter's stemming algorithm and the C libstemmer library generated by Snowball)
  # relative pruning (remove features that appear in less than 0.5% and in more than 99% of all documents)
dfm <- do_preprocessing(dfm.combined.nd)

# convert dfm to stm format
stm <- convert(dfm, to = "stm")
save(stm, file = "data/stm_dfm.RData")

# determine optimal numbber of topics
determine_k <- searchK(stm$documents, stm$vocab, K = c(5,10,15,20,25,30,35,40), cores = 2)
pdf("tm/determine_k.pdf")
plot(determine_k)
dev.off()

# estimate stm (K = 25)
load("data/stm_dfm.RData")
stm.k25 <- selectModel(documents = stm$documents, vocab = stm$vocab, K = 25, runs = 20, max.em.its = 300, init.type = "LDA", seed = 1234, verbose = TRUE)
save(stm.k25, file = "tm/stm_k25.RData")

# select best model
pdf("tm/select_model_k25.pdf")
plotModels(stm.k25)
dev.off()
stm.k25.sel <- stm.k25$runout[[1]]
save(stm.k25.sel, file = "tm/stm_k25_select.RData")

# get topic topwords (n = 10)
topwords <- data.frame(t_id = paste0("t_", 1:25),
                       topwords = apply(sageLabels(stm.k25.sel, n = 10)$marginal$prob, 1, paste, collapse = ", "))
write.csv(topwords, file = "tm/topwords_k25.csv", row.names = FALSE)

# prepare documents for selection of topdocs
data_online <- url("https://campuscloud.unibe.ch:443/ssf/s/readFile/share/24850/8558498293618557092/publicLink/data_online.RData")
load(data_online)
data_offline <- url("https://campuscloud.unibe.ch:443/ssf/s/readFile/share/24081/-6801514032582368813/publicLink/data_offline.RData")
load(data_offline)
texts <- rbind(select(data.offdata, d_id, text),
               select(data.ondata, d_id, text))
texts <- filter(texts, d_id %in% names(stm$documents))
texts <- texts[match(names(stm$documents), texts$d_id), ]

# get topdocs (n = 10, thresh = 0.6)
topdocs <- get_topdocs(stm.k25.sel, texts, 10, 0.6)
write.csv(topdocs, file = "tm/topdocs_k25.csv", row.names = FALSE)
