##################################
## Prep data for topic modeling ##
##################################

ifelse(!require("tidyverse"), install.packages("tidyverse"), require(tidyverse))
ifelse(!require("quanteda"), install.packages("quanteda"), require(quanteda))
source("functions/make_corpus.R")
source("functions/dfm_splitgrams.R")
source("functions/do_preprocessing.R")

options(stringsAsFactors = FALSE)

###################
# prep online data

# load data
load("data/data_online_filtered.RData")
load("data/vocab_translated.RData")

# separate data set per language
pages_en <- filter(pages, language == "en")
pages_de <- filter(pages, language == "de")

# make a document-feature matrix (dfm)
corpus_en <- make_online_corpus(pages_en)
tokens_en <- tokens(corpus_en, what = 'word', remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_twitter = TRUE,
                    remove_url = TRUE, remove_numbers = TRUE, remove_hyphens = FALSE, include_docvars = TRUE, verbose = TRUE)
dfm_en <- dfm(tokens_en, tolower = FALSE, stem = FALSE, verbose = TRUE)

# translate english vocabulary to german
feat_en <- as.data.frame(featnames(dfm_en))
colnames(feat_en) <- "en_voc"
feat_translated <- feat_en %>%
  left_join(translated, by = "en_voc")

# put translated vocabulary back into dfm (all lower case)
dfm_en@Dimnames$features <- tolower(feat_translated$translated)

# split ngrams
dfm_en <- dfm_splitgrams(dfm_en, concatenator = " ")

# make a dfm for german documents
corpus_de <- make_online_corpus(pages_de)
tokens_de <- tokens(corpus_de, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_twitter = TRUE,
                    remove_url = TRUE, remove_numbers = TRUE, remove_hyphens = FALSE, include_docvars = TRUE, verbose = TRUE)
dfm_de <- dfm(tokens_de, tolower = TRUE, stem = FALSE, verbose = TRUE)

# combine german and "english" dfm
dfm_online <- dfm_compress(rbind(dfm_de, dfm_en), margin = "features")

# add docvars again
docvars_online <- rbind(dfm_de@docvars, dfm_en@docvars)
docvars_online <- docvars_online[match(dfm_online@Dimnames$docs, docvars_online$d_id),]
dfm_online@docvars <- docvars_online

# save online dfm (raw)
save(dfm_online, file = "data/dfm_online.RData")

# make online dfm without duplicates
dfm_online_nd <- dfm_subset(dfm_online, duplicate == 0)

# save online dfm withouth duplicates
save(dfm_online_nd, file = "data/dfm_online_nd.RData")

####################
# prep offline data

load("data/data_offline.RData")

# make a dfm for offline documents
corpus_offline <- make_offline_corpus(data.offdata)
tokens_offline <- tokens(corpus_offline, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE, remove_twitter = TRUE,
                         remove_url = TRUE, remove_numbers = TRUE, remove_hyphens = FALSE, include_docvars = TRUE, verbose = TRUE)
dfm_offline <- dfm(tokens_offline, tolower = TRUE, stem = FALSE, verbose = TRUE)

# save offline dfm (raw)
save(dfm.offline, file = "data/dfm_offline.RData")

#########################################
# combine online and offline data for tm

# combine online and offline dfm (all)
dfm_combined <- dfm_compress(rbind(dfm_online, dfm_offline), margin = "features")

# combine online and offline dfm (without duplicates)
dfm_combined_nd <- dfm_compress(rbind(dfm_online_nd, dfm_offline), margin = "features")

# save combined dfm (raw)
save(dfm_combined, dfm_combined_nd, file = "data/dfm_combined.RData")

###################
# do preprocessing

# in that order
## all to lower
## split ngrams
## removal of features with less than 3 characters
## removal of stopwords
## stemming (using Martin Porter's stemming algorithm and the C libstemmer library generated by Snowball)
## relative pruning (remove features that appear in less than 0.5% and in more than 99% of all documents)
dfm_prep <- do_preprocessing(dfm_combined_nd)

# convert dfm to stm format
dfm_prep_converted <- convert(dfm_prep, to = "stm")

save(dfm_prep_converted, file = "data/dfm_prep.RData")
